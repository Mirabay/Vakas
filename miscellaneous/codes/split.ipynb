{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN68tAiayi21xdNuVrRzT2k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHnBNwE2N9V-","executionInfo":{"status":"ok","timestamp":1728862466827,"user_tz":360,"elapsed":20057,"user":{"displayName":"Luis Ángel Cruz García","userId":"11387126111024198197"}},"outputId":"50a34dd0-3f6c-4336-be91-57611bd477d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["##k-fold cross validation"],"metadata":{"id":"Be42m42STmdz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSeyJaQHNC0K"},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Subset\n","from torchvision.datasets import ImageFolder\n","from sklearn.model_selection import KFold\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Definir transformaciones para las imágenes\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),  # Ajustar el tamaño\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normaliza los valores de pix\n","])\n","\n","# Cargar el conjunto de datos\n","dataset = ImageFolder(root='/Users/luisgc/Desktop/E/SP/luis', transform=transform)\n","\n","# Verifica que las clases están correctamente identificadas\n","print(f\"Clases encontradas: {dataset.classes}\")\n","\n","'''Modelo para prueba'''\n","\n","# Modelo simple (CNN) para clasificación de las tres clases\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv_layer = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2)\n","        )\n","        self.fc_layer = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(16 * 64 * 64, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 3)  # 3 clases: vaca_de_pie, vaca_acostada, cama_vacia\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_layer(x)\n","        x = self.fc_layer(x)\n","        return x\n","\n","''' k-fold cross validation '''\n","\n","# Definir el número de folds\n","k_folds = 5\n","\n","# Definir KFold para dividir los datos\n","kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n","\n","# Diccionario para almacenar los resultados de cada fold\n","results = {}\n","\n","# Hiperparametros\n","num_epochs = 5\n","learning_rate = 0.001\n","\n","# Ciclo para los folds\n","for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n","\n","    print(f'Fold {fold+1}/{k_folds}')\n","\n","    # Crear subsets para este fold\n","    train_subset = Subset(dataset, train_idx)\n","    test_subset = Subset(dataset, test_idx)\n","\n","    # DataLoader para entrenamiento y validación\n","    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n","    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n","\n","    # Inicializar el modelo, criterio (loss) y optimizador\n","    model = SimpleCNN()\n","    criterion = nn.CrossEntropyLoss()  # Para clasificación multiclase\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Entrenamiento por epocas\n","    for epoch in range(num_epochs):\n","        model.train()  # Modo de entrenamiento\n","        running_loss = 0.0\n","\n","        for images, labels in train_loader:\n","            optimizer.zero_grad()  # Resetear gradientes\n","            outputs = model(images)  # Forward\n","            loss = criterion(outputs, labels)  # Calcular pérdida\n","            loss.backward()  # Backpropagation\n","            optimizer.step()  # Actualizar pesos\n","\n","            running_loss += loss.item()\n","\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n","\n","    # Evaluación en el conjunto de validacion\n","    model.eval()  # Modo de evaluacion\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # No se calculan gradientes en evaluación\n","        for images, labels in test_loader:\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)  # Predicción\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print(f'Fold {fold+1} Accuracy: {accuracy}%')\n","\n","    # Guardar los resultados de este fold\n","    results[fold] = accuracy\n","\n","# Resultados finales\n","print(\"\\nFinal Cross-Validation Results:\")\n","for fold in range(k_folds):\n","    print(f\"Fold {fold+1}: {results[fold]}%\")\n","print(f'Average Accuracy: {sum(results.values())/len(results)}%')"]},{"cell_type":"markdown","source":["##Split en carpetas"],"metadata":{"id":"qbGy_PDBpGr2"}},{"cell_type":"code","source":["import os\n","import shutil\n","import numpy as np\n","\n","# Configuraciones de directorios\n","base_dir = '/Users/luisgc/Desktop/dataset_vacas'\n","train_dir = '/Users/luisgc/Desktop/train'\n","val_dir = '/Users/luisgc/Desktop/validation'\n","test_dir = '/Users/luisgc/Desktop/test'\n","\n","# Crear directorios de salida\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(val_dir, exist_ok=True)\n","os.makedirs(test_dir, exist_ok=True)\n","\n","classes = ['vaca_de_pie', 'vaca_acostada', 'cama_vacia']\n","\n","# Separar datos\n","for class_name in classes:\n","    class_path = os.path.join(base_dir, class_name)\n","    images = os.listdir(class_path)\n","    np.random.shuffle(images)  # Mezclar las imágenes\n","\n","    # Calcular los tamaños de los conjuntos\n","    n_total = len(images)\n","    n_train = int(n_total * 0.7)\n","    n_val = int(n_total * 0.15)\n","\n","    # Separar las imágenes\n","    train_images = images[:n_train]\n","    val_images = images[n_train:n_train + n_val]\n","    test_images = images[n_train + n_val:]\n","\n","    # Crear las carpetas para cada conjunto\n","    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n","    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n","    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n","\n","    # Mover las imágenes a sus respectivas carpetas\n","    for img in train_images:\n","        shutil.copy(os.path.join(class_path, img), os.path.join(train_dir, class_name, img))\n","    for img in val_images:\n","        shutil.copy(os.path.join(class_path, img), os.path.join(val_dir, class_name, img))\n","    for img in test_images:\n","        shutil.copy(os.path.join(class_path, img), os.path.join(test_dir, class_name, img))\n","\n","# Imprimir el total de imágenes en cada carpeta\n","def print_image_counts(base_dir, classes):\n","    for class_name in classes:\n","        train_count = len(os.listdir(os.path.join(train_dir, class_name)))\n","        val_count = len(os.listdir(os.path.join(val_dir, class_name)))\n","        test_count = len(os.listdir(os.path.join(test_dir, class_name)))\n","        print(f\"Clase: {class_name}\")\n","        print(f\"  Entrenamiento: {train_count}\")\n","        print(f\"  Validación: {val_count}\")\n","        print(f\"  Prueba: {test_count}\")\n","        print(\"\")\n","\n","print_image_counts(base_dir, classes)"],"metadata":{"id":"dHkMGhI6pFy1"},"execution_count":null,"outputs":[]}]}